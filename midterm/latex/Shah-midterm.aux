\relax 
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Code}{1}}
\@writefile{lol}{\contentsline {lstlisting}{characterRNN.py}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Part 1: Hidden Units}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hidden Units effect on Perplexity}}{9}}
\newlabel{tbl:aTable}{{1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Part 1: Analysis}{9}}
\@writefile{toc}{\contentsline {paragraph}{ As the number of hidden units decreases, there is a marked drop in perplexity. Perplexity represents the difficulty the net has in predicting output. In this character RNN, the perplexity decreasing means that the network has an easier time predicting characters as the number of hidden units increases. These hidden units allow for more information to be factored into producing character output. The direct result of increasing hidden units is that the embedding, or vectorization of characters, is expanded by the increase in hidden units size. The effect is apparent from the table 1. }{9}}
\@writefile{toc}{\contentsline {paragraph}{ Looking at the sentences produced by the lowest number of hidden units, 32, almost none of the words produced are English words. Very few of the sentences are readable, and even fewer follow any semblance of sentence structure. Compared to the higher hidden unit output, this seems almost random even by the end of the training. At 512 hidden units, most if not all words by the end of training are either English words or close enough approximations. Punctuation and symbols are used in ways that seem to mirror the source material, such as a character name and colon followed by a newline preceding what must be that character's dialogue. The spelling, syntax, and overall structure of sentences produced are much more readable and often wholly correct with higher hidden units. }{9}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Part 2: Sequence Length}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Sequence Length effect on Perplexity}}{10}}
\newlabel{tbl:bTable}{{2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Part 2: Analysis}{10}}
\@writefile{toc}{\contentsline {paragraph}{ The sequence length is how many characters the RNN takes into account for modeling prediction. The output and states from longer or shorter sequences affects the predictions formed, but not the ease at which the net can predict them. Thus, the perplexity from 25 to 75 has very little change. }{10}}
\@writefile{toc}{\contentsline {paragraph}{ We can see that with a shorter sequence length the contextual accuracy of sentences, such as forming valid English words and structure, are diminished. In comparison, the longer sequence length is comprehensible as a fragment of a statement, albeit grammar structure is not entirely possible with character based prediction. This demonstrates the sequence length's effect on context and accuracy beyond predicting just a character, the amount of input affects how well that outputs fits overall in a human readable sentence. It is still just as difficult (noted by the perplexity remaining the same) to predict the characters, but the "understanding" that comes with a longer sequence length is reflected in the output text. }{10}}
