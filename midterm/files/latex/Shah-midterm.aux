\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Code}{1}{section.0.1}}
\@writefile{lol}{\contentsline {lstlisting}{characterRNN.py}{1}{lstlisting.0.-1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Part 1: Hidden Units}{9}{section.0.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hidden Units effect on Perplexity}}{9}{table.0.1}}
\newlabel{tbl:aTable}{{1}{9}{Hidden Units effect on Perplexity}{table.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Part 1: Analysis}{9}{subsection.0.2.1}}
\@writefile{toc}{\contentsline {paragraph}{ As the number of hidden units increases while the sequence length remains the same, there is a marked drop in perplexity. Perplexity represents the difficulty the net has in predicting output. In this character RNN, the perplexity decreasing means that the network has an easier time predicting characters. Increasing the hidden units allows for more information to be factored into producing character output. The direct result of increasing hidden units is that the embedding, or vectorization of characters, is expanded by the increase in hidden units size. By decreasing perplexity we can also say that the model generalizes better with additional hidden units. The effect this has on perplexity is apparent in Table 1. }{9}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{ Looking at the sentences produced by the lowest number of hidden units, 32, almost none of the words produced are English words. Very few of the sentences are readable, and even fewer follow any semblance of sentence structure. This seems more random than predictive even by the end of training. Comparatively, at 512 hidden units, most if not all words are either English words or close approximations. Punctuation and symbols are used in ways that seem to mirror the source material, such as a character name and colon followed by a newline preceding what must be that character's dialogue. The spelling, syntax, and overall structure of sentences produced are much more readable and some later epochs are wholly correct. This marked improvement can be attributed to higher hidden units. }{9}{section*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Part 2: Sequence Length}{10}{section.0.3}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Sequence Length effect on Perplexity}}{10}{table.0.2}}
\newlabel{tbl:bTable}{{2}{10}{Sequence Length effect on Perplexity}{table.0.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Part 2: Analysis}{10}{subsection.0.3.1}}
\@writefile{toc}{\contentsline {paragraph}{ Next I varied the sequence length while keeping the number of hidden units constant at 128. The sequence length is how many characters the RNN takes into account for modeling prediction and context. The output and states from longer or shorter sequences affect the predictions formed, but not the ease at which the net can predict them. Thus, using 25, 50, and 75 as the sequence length has no effect on perplexity directly. A sequence length of 75 is no better able to predict characters than at 25, however the difference is reflected in the output produced by the net. }{10}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{ We can see that with a shorter sequence length the contextual accuracy of sentences, such as forming valid English words and structure, are diminished. In comparison, the longer sequence length is comprehensible, it seems like a line right out of the source material. However, grammar structure is not fully formed with so few epochs. This demonstrates the sequence length's effect on context and accuracy beyond just predicting the next character. The sequence length affects how well that output reads, where a longer sequence length better captures context or patterns of the input. It is still just as difficult (the perplexity remains the same) to predict the characters, but the "understanding" that comes with a longer sequence length is reflected in the output text. }{10}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.2}Part 3: 50 Sequences of Grey}{11}{subsection.0.3.2}}
\@writefile{toc}{\contentsline {paragraph}{I decided to run this on a dataset consisting of the entire 50 Shades of Grey novel. For your enjoyment, computer generated romance:}{11}{section*.5}}
