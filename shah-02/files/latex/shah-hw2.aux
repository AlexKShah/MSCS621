\relax 
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Code}{1}}
\@writefile{lol}{\contentsline {lstlisting}{mnist3.py}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Results}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Neurons and Learning Rate Test Accuracy}}{4}}
\newlabel{tbl:aTable}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Explanation}{4}}
\@writefile{toc}{\contentsline {paragraph}{ As the number of neurons increases, the accuracy increases. It is also clear the best performance for learning rate follows a goldilocks zone, varying with the neuron count. This zone is where the learning rate is not too small that the steps are inefficient, but not too large that it's leaping past finding its mark. }{4}}
\@writefile{toc}{\contentsline {paragraph}{ Another interesting observation is that neuron count affects where this goldilocks zone occurs with the learning rate. A smaller number of neurons has better accuracy when paired with a larger learning rate. As the number of neurons increases, we can see the best accuracy shift towards smaller learning rates. }{4}}
\@writefile{toc}{\contentsline {paragraph}{ While a larger number of hidden units can more effectively use smaller learning rates, m GPU could only run up to 2048 hidden units before running out of memory. At 2048 hidden units it took 122 seconds to complete each pass. The best accuracy at 2048 hidden units was 0.9707. However, at 32 hidden units the model achieved a comparable accuracy of 0.9703 but only took 16 seconds to run. While increasing the hidden units was sometimes able to provide better accuracy, the advantage to using a very high number of hidden units is lost to time and is computationally intesive. Overall, continuing to increase the number of hidden units can be detrimental due to the exploding/vanishing gradient problem as well as time and computation complexity. }{4}}
