\relax 
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Code}{1}}
\@writefile{lol}{\contentsline {lstlisting}{mnist3.py}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Results}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Neurons and Learning Rate Test Accuracy}}{4}}
\newlabel{tbl:aTable}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Explanation}{4}}
\@writefile{toc}{\contentsline {paragraph}{ We can see as the number of neurons increases the accuracy tends to rise. As another general case we can see that the best performance for learning rate is a goldilocks zone, varying with the neuron count, where the learning rate is not too small that the steps are inefficient, but not too large that it's leaping past finding it's mark. }{4}}
\@writefile{toc}{\contentsline {paragraph}{ What's also interesting is how neuron count affects a larger learning rate. A smaller number of neurons has better accuracy when paired with a larger learning rate, compared to larger number of neurons with the same larger learning rate. Conversely, as the number of neurons increases, we can see the best accuracy shift towards smaller learning rates. }{4}}
\@writefile{toc}{\contentsline {paragraph}{ A very large number of hidden units can take advantage of very tiny steps in the smallest learning rates. Whereas less hidden units requires a larger learning rate. My GPU could only run up to 2048 hidden units before running out of memory, and it took 122 seconds to complete each pass. The best accuracy with 2048 hidden units was 0.9707. However, 32 hidden units achieved a comparable accuracy of 0.9703 which only took 16 seconds to run. While increasing the hidden units was sometimes able exceed this value, the advantage to using a very high number of hidden units is lost to time and is computationally intesive and can overall be inneffective due to the exploding/vanishing gradient problem. }{4}}
