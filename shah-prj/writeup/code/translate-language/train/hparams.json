{"symbol_modality_num_shards": 16, "initializer_gain": 1.0, "kernel_width": 1, "daisy_chain_variables": true, "prepend_mode": "none", "relu_dropout": 0.1, "num_encoder_layers": 0, "learning_rate_cosine_cycle_steps": 250000, "target_modality": "default", "max_relative_position": 0, "min_length": 0, "weight_noise": 0.0, "moe_k": 2, "use_fixed_batch_size": false, "initializer": "uniform_unit_scaling", "num_hidden_layers": 6, "attention_dropout": 0.1, "norm_type": "layer", "parameter_attention_value_channels": 0, "filter_size": 2048, "optimizer": "Adam", "dropout": 0.2, "self_attention_type": "dot_product", "input_modalities": "default", "scheduled_sampling_gold_mixin_prob": 0.5, "optimizer_momentum_momentum": 0.9, "num_heads": 8, "norm_epsilon": 1e-06, "pos": "timing", "learning_rate_decay_scheme": "noam", "compress_steps": 0, "hidden_size": 512, "attention_key_channels": 0, "problem_choice": "adaptive", "moe_loss_coef": 0.01, "kernel_height": 3, "data_dir": "data/", "nbr_decoder_problems": 1, "layer_postprocess_sequence": "da", "moe_num_experts": 64, "max_target_seq_length": 0, "optimizer_adam_epsilon": 1e-09, "grad_noise_scale": 0.0, "scheduled_sampling_prob": 0.0, "length_bucket_step": 1.1, "learning_rate_warmup_steps": 16000, "moe_hidden_sizes": "2048", "scheduled_sampling_warmup_steps": 50000, "use_tpu": false, "shared_embedding_and_softmax_weights": true, "num_decoder_layers": 0, "layer_prepostprocess_dropout": 0.1, "max_length": 256, "clip_grad_norm": 0.0, "use_pad_remover": true, "proximity_bias": false, "factored_logits": false, "eval_drop_long_sequences": false, "multiply_embedding_mode": "sqrt_depth", "optimizer_adam_beta1": 0.9, "symbol_modality_skip_top": false, "sampling_temp": 1.0, "label_smoothing": 0.1, "optimizer_adam_beta2": 0.98, "parameter_attention_key_channels": 0, "batch_size": 2048, "weight_decay": 0.0, "attention_value_channels": 0, "min_length_bucket": 8, "max_input_seq_length": 0, "summarize_grads": false, "learning_rate": 0.2, "sampling_method": "argmax", "ffn_layer": "dense_relu_dense", "tpu_batch_size_per_shard": 24, "layer_preprocess_sequence": "n"}